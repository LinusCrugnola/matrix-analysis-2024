{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833d835",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"QR decomposition.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d71c2-7b02-4185-9a08-5dc10d903dad",
   "metadata": {},
   "source": [
    "# Matrix Analysis 2024 - EE312\n",
    "\n",
    "## Week 12 - QR DECOMPOSITION\n",
    "[LTS2](https://lts2.epfl.ch)\n",
    "\n",
    "### Objectives\n",
    "The goal of this week's exercise session is to study some aspects of the QR decomposition and its connections to projections and inverse. We will study two methods that can be used to compute the QR decomposition of a matrix and an application to eigendecomposition.\n",
    "\n",
    "Please submit your answers **individually**.\n",
    "\n",
    "Let us consider a square $n\\times n$ real matrix $A$ having linearly independent columns (NB: QR factorization is applicable to rectangular matrices, we consider square matrices for simplification). The QR decomposition of $A$ is $A=QR$ where $Q$ is an orthogonal matrix and $R$ and upper-triangular matrix.\n",
    "\n",
    "\n",
    "\n",
    "# 1. Gram-Schmidt orthogonalization\n",
    "\n",
    "Reminder:\n",
    "- The projection operator of a vector $v$ on another $u$ is given by $P_u v = \\frac{\\langle u,v \\rangle}{\\langle u,u \\rangle}u$.\n",
    "- Gram-Schmidt orthogonalization process of a basis $(v_1, v_2, ..., v_{n})$ produces an orthormal basis $(e_1, e_2, ..., e_{n})$ as follows:\n",
    "\n",
    "$u_1 = v_1$, $e_1 = \\frac{u_1}{\\|u_1\\|}$\n",
    "\n",
    "$u_2 = v_2 - P_{u_1}v_2$, $e_2 = \\frac{u_2}{\\|u_2\\|}$\n",
    "\n",
    "$u_3 = v_3 - P_{u_2}v_3 - P_{u_1}v_3$, $e_3 = \\frac{u_3}{\\|u_3\\|}$\n",
    "\n",
    "...\n",
    "\n",
    "$u_k = v_k - \\sum_{j=1}^{k-1} P_{u_j}v_{k}$, $e_k = \\frac{u_k}{\\|u_k\\|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2397ef-938a-46da-a0c9-1a78fa322046",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 1.1 \n",
    "We perform the QR decomposition of $A$ by applying Gram-Schmidt to its column vectors of $A$ (denoted by $a_k, k=1,...n)$. $Q$ is made of the column vectors $e_k$, i.e. $Q=(e_1|e_2|...|e_{n})$. Let us denote by $r_{jk}$ the coefficients of $R$.\n",
    "\n",
    "Prove that $r_{kk} = \\|u_k\\|$, $r_{jk} = <e_j, a_k>$ for $j<k$ and $r_{jk} = 0$ for $j>k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ea06fc",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e301db37-b940-4da2-a2f4-4ce603a881e1",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "Using the Gram-Schmidt formula, we can write:\n",
    "$a_k = u_k + \\sum_{j=1}^{k-1} P_{u_j}a_{k} = u_k + \\sum_{j=1}^{k-1}\\frac{<u_j, a_k>}{\\|u_j\\|^2}u_j = \\|u_k\\|e_k + \\sum_{j=1}^{k-1}\\frac{<u_j, a_k>}{\\|u_j\\|^2}\\|u_j\\|e_j = \\|u_k\\|e_k + \\sum_{j=1}^{k-1}\\frac{<u_j, a_k>}{\\|u_j\\|}e_j = \\|u_k\\|e_k + \\sum_{j=1}^{k-1}<e_j, a_k>e_j $\n",
    "\n",
    "Using the definition of $Q$ and $R$ we also have $a_k = \\sum_{j=1}^n r_{jk} e_j$. Using the equality above, this proves that $r_{kk} = \\|u_k\\|$, $r_{jk} = <e_j, a_k>$ for $j<k$ and $r_{jk} = 0$ for $j>k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93abc7b-3142-4515-a03e-a7b582af3815",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 1.2\n",
    "Implement a function that performs the QR decomposition of a square $n\\times n$ matrix using the Gram-Schmidt orthogonalization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0b053a5-e135-49c5-8d8e-bcb3a40a1970",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c455d1c2-2f2c-4316-81e5-7eb202f87946",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def qr_decomp_gs(A):\n",
    "    \"\"\"\n",
    "    Performs the QR decomposition of the matrix using Gram-Schmidt\n",
    "    \n",
    "    Parameters: \n",
    "      - A: input matrix nxn\n",
    "      \n",
    "    Returns:\n",
    "      - Q matrix (orthogonal)\n",
    "      - R matrix (upper triangular)\n",
    "    \"\"\"\n",
    "\n",
    "    n = A.shape[0]\n",
    "    Q = np.zeros((n,n))\n",
    "    R = np.zeros((n,n))\n",
    "    # BEGIN SOLUTION\n",
    "    U = np.zeros((n,n))\n",
    "    U[:, 0] = A[:, 0]\n",
    "    Q[:, 0] = A[:, 0]/np.linalg.norm(A[:, 0])\n",
    "    R[0, 0] = np.linalg.norm(A[:, 0])\n",
    "    for k in np.arange(1, n):\n",
    "        acc = np.zeros((n))\n",
    "        for j in np.arange(0, k):\n",
    "            R[j, k] = np.dot(A[:, k], Q[:, j])\n",
    "            acc = acc + (np.dot(A[:, k], U[:, j])/np.dot(U[:, j], U[:, j]))*U[:, j]\n",
    "\n",
    "        U[:, k] = A[:, k] - acc\n",
    "        Q[:, k] = U[:, k]/np.linalg.norm(U[:, k])\n",
    "        R[k, k] = np.linalg.norm(U[:, k])\n",
    "    return Q,R\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548f4f3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1p2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3460934-a3b6-4c79-bd61-f1eeac645f73",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 1.3\n",
    "Using the properties of Q, write a routine that can solve a linear system $Ax=b$ using the QR factorization of $A$ ($A$ being a $n\\times n$ real matrix).\n",
    "What property do the coefficients of $R$ need to satisfy for the solution to exist ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d826d398-835e-4dc4-82a8-5fbe0a0fcc40",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "We have $Ax = b$ and $A=QR$ with $Q$ orthogonal hence $Q^TQRx = Q^Tb$ i.e. $Rx=Q^Tb$. \n",
    "\n",
    "Given $R$ is upper triangular, we do not need to perform compute $R^{-1}$ explicitely and the system can be solved using backsubstitution. \n",
    "Since we divide by $r_{kk}$ to get the $x_k$ values, those diagonal coefficients have to be non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "00940be7-8e7b-42f9-a691-ea4631a235e2",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def solve_gs(A, b):\n",
    "    \"\"\"\n",
    "    Solve a linear system Ax=b using QR decomposition\n",
    "    \n",
    "    Parameters:\n",
    "      - A input matrix nxn\n",
    "      - b result vector (length n)\n",
    "    \n",
    "    Returns:\n",
    "      - solution vector x (length n)\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    x = np.zeros(n)\n",
    "    # BEGIN SOLUTION\n",
    "    Q, R = qr_decomp_gs(A)\n",
    "    c = Q.T@b\n",
    "    x = np.zeros(n)\n",
    "    x[n-1] = c[n-1]/R[n-1, n-1]\n",
    "    for k in np.arange(n-2, -1, -1):\n",
    "        s = np.dot(R[k, k+1:], x[k+1:])\n",
    "        x[k] = (c[k] - s)/R[k,k]\n",
    "    return x\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ed57911-5f2e-4953-b2ce-26dd669daa84",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Small numerical example\n",
    "A=np.array([[12,-51,4],[6,167,-68],[-4,24,-41]])\n",
    "b=np.array([1, -1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9fd4a98d-cdc8-4c09-914c-0e95fc0483ca",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "x = solve_gs(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797892aa-a084-4524-8f74-545094e5bdcf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "A@x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "488c1a2a-1735-4d3b-9a25-2a496c031dc0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39362e95",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1p3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d1ec0-f35b-4ff0-9b69-feeb56a7bac0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 1.4\n",
    "Consider the matrix \n",
    "$B=\\begin{pmatrix}1 & 1 & 1 \\\\ \\varepsilon & 0 & 0\\\\ 0 & \\varepsilon & 0\\end{pmatrix}$, with $\\varepsilon$ being small enough (typically $10^{-8}$ or smaller). In that case, is the method used to compute Q and R still reliable (and why) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8015bbb0-2d23-4e57-bbb9-3feba8d4e7c8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "eps=1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25058499",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d39aa-1c4a-4d22-bf74-5bd51fcc2563",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "B = np.array([[1,1,1],[eps,0,0],[0,eps,0]])# SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cdacf1-a8d7-4b80-be17-036fc0263212",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the orthogonality of Q\n",
    "QB@QB.T # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e866b116-404c-4ab6-8e69-bb7caa5ff46b",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the validity of the decomposition\n",
    "QB@RB-B # SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c89cbf-3358-4409-9803-34f85fe4f00d",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "We still have $QR=B$, but $Q$ is no longer orthogonal ! This is due to the numerical precision of the computer, that rounds off the norm of the first column of $B$ to 1, when it should be $\\sqrt{1+\\varepsilon^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ec7260-2cda-4990-b08a-2cc45e17c103",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# 2. Householder reflections\n",
    "\n",
    "We will now study and implement another method to perform the QR decomposition that is more resistant to the numerical issues mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21765803-4126-4b71-84b9-5c77ac3137cc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 2.1 \n",
    "Let $v$ be a unit vector and the associated transform $H_v=I-2vv^T$. Prove that $H_v$ is orthogonal and that it preserves the norm. What is the effect of $H_v$ on a vector $u$ orthogonal to $v$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254fd21b",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f7184-32f2-480e-adec-877ce846d161",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "$H_vH_v^T = (I - 2vv^T)(I - 2vv^T)^T = (I - 2vv^T)(I - 2vv^T) = I -4vv^T + 4v(v^Tv)v^T$\n",
    "\n",
    "Since $v$ is a unit vector, $v^Tv=1$ i.e. $H_vH_v^T = I - 4vv^T + 4vv^T = I$.\n",
    "\n",
    "$\\|H_vu\\|^2 = (H_vu)^TH_vu = u^TH_v^TH_vu =  u^Tu = \\|u\\|^2$, $H_v$, as any orthogonal transform preserves norm. \n",
    "\n",
    "If $u$ is orthogonal to $v$, we have $v^Tu=0$.\n",
    "$H_vu = u - 2 vv^Tu = u$. $H_v$ does not modify vectors orthogonal to $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5352bba-8353-443b-aa53-249b0341fdc2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 2.2 \n",
    "What is the effect of $H_v$ on any vector $u$ (drawing the planar case can be of help)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e13357",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57509402-a225-478b-a41c-966ab2eabec5",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "$H_vu = u - 2vv^Tu = u - 2<v,u>v$\n",
    "\n",
    "The term $<v,u>v$ is in fact the orthogonal projection of $u$ on $v$ (remember $<v,v>=\\|v\\|=1)$. If we draw the effect of $H_v$ on $u$, it yields the reflection of $u$ around $v^\\perp$.\n",
    "\n",
    "![householder](images/householder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b255fc-51a6-4f49-ab27-e00ea87e8394",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 2.3\n",
    "The goal is now to design $n$ reflection operators $H_{v_1}, H_{v_2}, ..., H_{v_n}$ s.t. we have\n",
    "$$\n",
    "H_{v_n}...H_{v_2}H_{v_1}A = R,\n",
    "$$\n",
    "where $R$ is an upper triangular matrix. \n",
    "\n",
    "The reflection operations are meant to be \"progressive\", i.e. $H_{v_k}...H_{v_1}A$ has its $k$ first columns upper triangular.\n",
    "\n",
    "- Find $v_1$ and $\\alpha$ s.t. $H_{v_1}a_1 = \\alpha e_1$, where $a_1$ is the first column of $A$ and $e_1 = \\begin{pmatrix}1\\\\0\\\\ \\vdots \\\\0 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e5ba6-ab4f-42f8-b4ac-5d0ddebef1d1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "- Let us now generalize the previous step to find $H_{v_k}$. Given a vector $x = \\begin{pmatrix}x^U \\\\ x^L\\end{pmatrix}$ with $x^U \\in\\mathbb{R}^k$\n",
    "and $x^L\\in\\mathbb{R}^{n-k}$. \n",
    "\n",
    "Find a vector $v_k$ s.t. $H_{v_k}x = \\begin{pmatrix}x^U \\\\ \\alpha e_1^L \\end{pmatrix}$, where $\\alpha\\in\\mathbb{R}$ and \n",
    "$e_1^L = \\begin{pmatrix}1\\\\0\\\\ \\vdots \\\\0 \\end{pmatrix} \\in \\mathbb{R}^{n-k}$. \n",
    "\n",
    "It might be of help to write $v_k=\\begin{pmatrix}v_k^U\\\\v_k^L\\end{pmatrix}$, with $v_k^U \\in\\mathbb{R}^k$\n",
    "and $v_k^L\\in\\mathbb{R}^{n-k}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d2bae",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f784608-692e-49f4-9ba2-46ba114f53e5",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "- $H_{v_1}a_1 = (I -2v_1v_1^T)a_1 = \\alpha e_1$, i.e. $a_1 - 2v_1v_1^Ta_1 = \\alpha e_1$, \n",
    "and finally $a_1 - \\alpha e_1 = 2 <v_1, a_1> v_1$, which implies that $v_1$ is colinear to $a_1 - \\alpha e_1$.\n",
    "Remembering that $H_{v_1}$ preserves norm, we have $\\|H_{v_1}a_1\\| = \\|a_1\\| = |\\alpha|$.\n",
    "Therefore $v_1 = \\frac{1}{\\|a_1 \\pm \\|a_1\\|e_1\\|}(a_1 \\pm \\|a_1\\|e_1)$ \n",
    "\n",
    "-  Since we have $H_{v_k} = I - 2v_kv_k^T = I - 2\\begin{pmatrix}v_k^U\\\\v_k^L\\end{pmatrix}\\begin{pmatrix}(v_k^U)^T & (v_k^L)^T\\end{pmatrix}$, which leads to $H_{v_k} = I - 2\\begin{pmatrix}v_k^U(v_k^U)^T & v_k^U(v_k^L)^T \\\\ (v_k^U)^Tv_k^L & v_k^L(v_k^L)^T\\end{pmatrix}$\n",
    "\n",
    "$H_{v_k}x = \\begin{pmatrix}x^U \\\\ x^L\\end{pmatrix} - 2\\begin{pmatrix}v_k^U(v_k^U)^T & v_k^U(v_k^L)^T \\\\ (v_k^U)^Tv_k^L & v_k^L(v_k^L)^T\\end{pmatrix}\\begin{pmatrix}x^U \\\\ x^L\\end{pmatrix}$\n",
    "\n",
    "If we want the upper part of $x$ to be preserved, it is relatively obvious that $v_k^U = 0$ will be needed, which leads to\n",
    "$H_{v_k}x = \\begin{pmatrix}x^U \\\\ x^L\\end{pmatrix} - 2\\begin{pmatrix}0 & 0 \\\\ 0 & v_k^L(v_k^L)^T\\end{pmatrix}\\begin{pmatrix}x^U \\\\ x^L\\end{pmatrix}$.\n",
    "\n",
    "The lower part yields: $x^L - 2 v_k^L(v_k^L)^Tx^L = x^L - 2<v_k^L, x^L>v_k^L  = \\alpha e_1^L$. As for $v_1$, we see that $v_k$ is colinear to $x^L - \\alpha e_1^L$, and the norm preservation property yields $\\alpha = \\pm\\|x^L\\|$.\n",
    "\n",
    "Finally, $v_k^L = \\frac{1}{\\|x^L \\pm \\|x^L\\|e_1^L \\|}(x^L \\pm \\|x^L\\|e_1^L)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c386e-e498-4b85-ac8a-f3bea7230c6e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 2.4\n",
    "We now have all we need to compute the $H_{v_k}$ matrices. Implement the QR decomposition using the above results. \n",
    "You should have found that the sign of $\\alpha$ can be either positive or negative. Choose $\\alpha$ to have the sign opposite to the one of $x^L_k[0]$. (Use the `numpy.sign`function).\n",
    "\n",
    "Try your implementation on the $B$ matrix seen in ex. 1. What is the advantage of this implementation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13bdb8-5046-42e4-a3e0-f58654acce81",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def qr_decomp_hh(A):\n",
    "    \"\"\"\n",
    "    Performs the QR decomposition of the matrix using Householder reflections\n",
    "    \n",
    "    Parameters: \n",
    "      - A: input matrix nxn\n",
    "      \n",
    "    Returns:\n",
    "      - Q matrix (orthogonal)\n",
    "      - R matrix (upper triangular)\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    # BEGIN SOLUTION\n",
    "    E = np.eye(n)\n",
    "    Q = np.eye(n)\n",
    "    R = A.copy()\n",
    "    for k in np.arange(0, n):\n",
    "        u = np.zeros(n)\n",
    "        s = np.sign(R[k,k])\n",
    "        u[k:] = R[k:,k] + s*np.linalg.norm(R[k:,k])*E[k:, k]\n",
    "        v = u/np.linalg.norm(u)\n",
    "        H = E - 2*np.outer(v, v)\n",
    "        R = H@R\n",
    "        Q = H@Q\n",
    "    return Q.T,R # in this case we need the 'inverse' of Q\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc670d-b80e-4ad0-bf4a-54ec5bc9aa8d",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "qh, rh = qr_decomp_hh(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad58b8-9d20-4a25-81b7-ff8c5684d0b3",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Check orthogonality of Q\n",
    "qh@qh.T # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211740b-24f4-4408-8bb9-7d83ca0f46f8",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Check validity of solution\n",
    "qh@rh-B # SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d869f-f2c7-40eb-b204-d593b6ee6133",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "The QR factorization using the Householder reflections is now giving correct results on $B$. This seems a better choice in terms of numerical stability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fafb41-cc9d-4936-bbac-18b60797f66d",
   "metadata": {},
   "source": [
    "# 3. Eigendecomposition and QR\n",
    "\n",
    "We will now study some aspects of how to actually perform (numerically) the eigendecomposition of a matrix. \n",
    "\n",
    "In the following we will suppose that $A$ has distinct eigenvalues, and we will write them as $\\lambda_1, ..., \\lambda_n$ s.t. $|\\lambda_1| > |\\lambda_2|...>|\\lambda_n|$, and denote their associated normalized eigenvectors as $v_1, v_2..., v_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05429ad4-8b35-47d5-ad98-3ccc8300a658",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 3.1 Power method\n",
    "\n",
    "Let us consider a vector $w\\in\\mathbb{R}^n$ of norm 1, and its associated eigendecomposition $w = \\sum_{k=1}^n\\alpha_k v_k$. \n",
    "\n",
    "Compute $A^pw$ (for $p\\in\\mathbb{N}$, $k>0$). Assuming that $\\alpha_1\\neq 0$ for the chosen $w$, use this result to find an algorithm to compute $\\lambda_1$ and $v_1$ (you may want to introduce another vector $x\\in\\mathbb{R}^n$ s.t. $x^Tv_1\\neq 0$ and consider the quantity $x^TA^pw$) ?\n",
    "\n",
    "How would you proceed for $\\lambda_2$ and $v_2$ and the other eigenvalues ?\n",
    "\n",
    "What are the limitations of this method ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e45de",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33732ae1-4a27-4edd-8944-0ddfc806958b",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "Since we have $Av_k = \\lambda_kv_k$, we have $Aw = \\sum_{k=1}^n\\alpha_k\\lambda_kv_k$. If we repeat the operation, we easily get $A^pw = \\sum_{k=1}^n\\alpha_k\\lambda_k^pv_k$, which in turn can be rewritten as \n",
    "$$\n",
    "A^pw = \\lambda_1^p \\left[\\alpha_1v_1 + \\sum_{k=2}^n\\alpha_k\\left(\\frac{\\lambda_k}{\\lambda_1}\\right)^pv_k\\right]\n",
    "$$\n",
    "\n",
    "Since $\\left|\\frac{\\lambda_k}{\\lambda_1}\\right|<1$ for $k>1$, when $p\\to+\\infty$ those terms will vanish. While this is interesting, this would not be directly applicable to estimate $\\lambda_1$, since the power of $\\lambda_1$ itself can grow (or vanish) quickly.\n",
    "\n",
    "If we consider the scalar $x^TA^pw$, we have\n",
    "$$\n",
    "x^TA^pw = \\lambda_1^p\\left[\\alpha_1x^Tv_1 + \\sum_{k=2}^n\\alpha_k\\left(\\frac{\\lambda_k}{\\lambda_1}\\right)^px^Tv_k\\right]\n",
    "$$\n",
    "In order to have estimate $\\lambda_1$ we can consider\n",
    "\\begin{align*}\n",
    "\\frac{x^TA^pw}{x^TA^{p-1}w} &= \\lambda_1\\frac{\\left[\\alpha_1x^Tv_1 + \\sum_{k=2}^n\\alpha_k\\left(\\frac{\\lambda_k}{\\lambda_1}\\right)^px^Tv_k\\right]}{\\left[\\alpha_1x^Tv_1 + \\sum_{k=2}^n\\alpha_k\\left(\\frac{\\lambda_k}{\\lambda_1}\\right)^{p-1}x^Tv_k\\right]}\\\\\n",
    "& = \\lambda_1\\frac{\\left[1 + \\frac{1}{\\alpha_1x^Tv_1}\\sum_{k=2}^n\\alpha_k\\left(\\frac{\\lambda_k}{\\lambda_1}\\right)^px^Tv_k\\right]}{\\left[1 + \\frac{1}{\\alpha_1x^Tv_1}\\sum_{k=2}^n\\alpha_k\\left(\\frac{\\lambda_k}{\\lambda_1}\\right)^{p-1}x^Tv_k\\right]},\n",
    "\\end{align*}\n",
    "this quantity will converge towards $\\lambda_1$ (provided $x$ and $w$ match the requirements).\n",
    "\n",
    "As for estimating $v_1$ we can use the fact that $A^pw\\approx \\lambda_1^p\\alpha_1v_1$ when $p$ is large enough, which is parallel to $v_1$. In order to avoid overflows we can modify slightly the iteration and normalize the estimated vector at each step. \n",
    "\n",
    "Let us denote by $v_1^{(p)}$ the estimation of $v_1$ at iteration $p$. The algorithm to estimate $v_1$ is as follows\n",
    "- initialize $v_1^{0}=\\frac{w}{\\|w\\|}$\n",
    "- $v_1^{(p)} = \\frac{Av_1^{(p-1)}}{\\|Av_1^{(p-1)}\\|}$\n",
    "\n",
    "\n",
    "Once we have estimated $\\lambda_1$ and $v_1$, we can repeat the process for $\\lambda_2$ and $v_2$ by chosing an initial vector $w_2 = w - \\left<w, v_1\\right>v_1$ which gives \n",
    "$$\n",
    "Aw_2 = \\sum_{k=2}^n\\alpha_k\\lambda_kv_k\n",
    "$$\n",
    "\n",
    "\n",
    "There are a number of issues with this method, the main ones being \n",
    "- the speed of convergence which can be reduced if $\\lambda_2$ is close to $\\lambda_1$ \n",
    "- numerical stability issues when having large or small eigenvalues.\n",
    "- the eigenvalues/eigenvectors are computed sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405a28b-fcae-48c6-89df-cb0ac25f9750",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 3.2 QR method\n",
    "\n",
    "### 3.2.1\n",
    "Prove that the matrix $B_1=(I-v_1v_1^T)A$ has eigenvalues $0, \\lambda_2, ..., \\lambda_n$, with corresponding eigenvectors $v_1, (I-v_1v_1^T)v_2, ..., (I-v_1v_1^T)v_n$. How about the matrices $B_2=(I-v_2v_2^T)(I-v_1v_1^T)A, B_3=(I-v_3v_3^T)(I-v_2v_2^T)(I-v_1v_1^T)A$, etc. when supposing that $v_1, v_2...$ are orthogonal ?\n",
    "\n",
    "Using question 1 (you can show the operator $I-v^Tv$ is indeed the basis operation of Gram-Schmidt), reformulate the following algorithm using the QR decomposition of $AV^{(k)}$ (denoted by $V^{(k+1)}R^{(k+1)}$):\n",
    "- Start with $v_1^{(0)}, v_2^{(0)}, ..., v_n^{(0)}=V^{(0)}$\n",
    "- Iterate for $k=0, 1, ... $\n",
    "  - $v_1^{(k+1)}=Av_1^{(k)}$ and normalize $v_1^{(k+1)}$\n",
    "  - $v_2^{(k+1)} = (I-v_1^{(k+1)}v_1^{(k+1)T})Av_1^{(k)}$ and normalize $v_2^{(k+1)}$\n",
    "  - ...\n",
    "  - $v_n^{(k+1)} = (I-v_{n-1}^{(k+1)}v_{n-1}^{(k+1)T})...(I-v_1^{(k+1)}v_1^{(k+1)T})Av_1^{(k)}$ and normalize $v_n^{(k+1)}$\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d895858",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aeaa7c-3fa5-4d3b-814f-dbecf5bbfe98",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "We have $(I-v_1v_1^T)Av_1 = \\lambda_1 v_1 - \\lambda_1 v_1 v_1^T v_1 = \\lambda_1v_1 - \\lambda_1v_1 = 0$, therefore $v_1$ is an eigenvector of $(I-v_1v_1^T)A$ associated with eigenvalue 0.\n",
    "\n",
    "For $k>1$, we have\n",
    "\\begin{align*}\n",
    "(I-v_1v_1^T)A(I-v_1v_1^T)v_k &= (I-v_1v_1^T)(A-Av_1v_1^T)v_k\\\\ &\n",
    "= (A  - v_1v_1^TA - Av_1v_1^T + v_1v_1^TAv_1v_1^T)v_k\\\\ \n",
    "&= (A -v_1v_1^TA - \\lambda_1v_1v_1^T+ \\lambda_1v_1v_1^Tv_1v_1^T)v_k\\\\\n",
    "&= (I-v_1v_1^T)Av_k \\\\\n",
    "&= \\lambda_k(I-v_1v_1^T)v_k\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Using a similar reasoning, matrix $B_2$ will have $v_2$ as eigenvector associated to eigenvalue 0, and since $v_1$ and $v_2$ are independant, the 0 eigenvalue will have a multiplicity of 2, its largest eigenvalue (in absolute value) becomes $\\lambda_3$. For $k>2$, the eigenvectors associated to $\\lambda_k$ will be $(I-v_2v_2^T)(I-v_1v_1^T)v_k$. With a similar reasoning $B_k$ will have a zero eigenvalue of multiplicity $k$ and its largest eigenvalue (in absolute value) will be $\\lambda_k$.\n",
    "\n",
    "\n",
    "The algorithm described is the same as question 1. Since we are dealing with unit vector, we can rewrite the Gram-Schmidt step as:\n",
    "$$\n",
    "v - P_uv = v -(u^Tv)u = v - u(u^T)v = (I-uu^T)v\n",
    "$$\n",
    "(since $u^Tv$ is a scalar, it is possible to move it around).\n",
    "\n",
    "This transforms the basis $(Av_1^{(k)}, Av_2^{(k)}..., Av_n^{(k)})$ into an orthonormal basis $(v_1^{(k+1}, v_2^{(k+1}, ..., v_n^{(k+1})$. Therefore the algorithm described can be rewritten as\n",
    "- Initialize $(v_1^{(0)}, v_2^{(0)}, ..., v_n^{(0)})=V^{(0)}$\n",
    "- Iterate for $k=0, 1, ... $\n",
    "  - Compute $AV^{(k)}$ and perform its QR decomposition $AV^{(k)}=V^{(k+1)}R^{(k+1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2168af1e-835a-45d0-b9ac-50eb763eec20",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.2.2\n",
    "\n",
    "How does the algorithm of question 3.2.1 relates to the one from question 3.1 ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2eafe7",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf1bad-bea7-4f27-b52f-52021b2e47f2",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "The method found at the previous question performs the power iteration simultaneously on all estimates of the eigenvectors (instead of one at a time), allowing a reduced number of computations (provided the method actually converges to the desired solution, which we will not prove here)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef48172f-8fe5-4501-94e9-22dc9ee85999",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.2.3\n",
    "\n",
    "Let us introduce the matrix $A^{(k)}=V^{(k)T}AV^{(k)}$, where $(v_1^{(k)}, v_2^{(k)}, ..., v_n^{(k)})=V^{(k)}$. Let us also introduce the QR decomposition of $A^{(k)}=Q^{(k+1)}R^{(k+1)}$.\n",
    "\n",
    "Prove that $A^{(k+1)} = R^{(k+1)}Q^{(k+1)}$. What is the interest of this sequence of matrices $A^{(k)}$ regarding the eigenvalues of $A$ ?\n",
    "\n",
    "Hint: remember the QR decomposition is unique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5fc2a3",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38032b31-f5be-4fff-8d4c-4b0639234dcf",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "We have $AV^{(k)} = V^{(k+1)}R^{(k+1)}$, with $V^{(k)}$ and $V^{(k+1)}$ being orthogonal, therefore\n",
    "$$\n",
    "A^{(k)}=V^{(k)T}V^{(k+1)}R^{(k+1)},\n",
    "$$\n",
    "and \n",
    "$$\n",
    "A^{(k+1)}=V^{(k+1)T}AV^{(k+1)} = V^{(k+1)T}AV^{(k)}V^{(k)T}V^{(k+1)} = V^{(k+1)T}V^{(k+1)}R^{(k+1)}V^{(k)T}V^{(k+1)} = R^{(k+1)}V^{(k)T}V^{(k+1)}.\n",
    "$$\n",
    "\n",
    "By unicity of the QR decomposition, and since $V^{(k)T}V^{(k+1)}$ is orthogonal (as product of orthogonal matrices) the first result becomes $A^{(k)}=Q^{(k+1)}R^{(k+1)}$, with $Q^{(k+1)}=V^{(k)T}V^{(k+1)}$.\n",
    "\n",
    "In the second relationship this becomes $A^{(k+1)}=R^{(k+1)}Q^{(k+1)}$.\n",
    "\n",
    "Matrices $A^{(k)}$ are similar to $A$ and have the same eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729e9855-33aa-452f-a3ab-57258120933d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 3.2.4\n",
    "\n",
    "Implement the algorithm that generates the sequence $A^{(k)}$. The diagonal elements of $A^{(k)}$ will converge towards the eigenvalues of $A$ (you do not need to prove this result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219bc1f2-0003-4b23-ab02-e665d7b81568",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def eigendecomposition_qr(A, num_iter=20):\n",
    "    \"\"\"\n",
    "    Computes the eigenvalues of A using the QR iteration method\n",
    "    \n",
    "    Parameters:\n",
    "      - A: input nxn matrix\n",
    "      - num_iter: number of QR iterations to perform\n",
    "      \n",
    "    Returns:\n",
    "      - a vector containing the eigenvalues\n",
    "    \"\"\"\n",
    "    Ak = A\n",
    "    # BEGIN SOLUTION\n",
    "    for k in range(num_iter):\n",
    "        q,r = qr_decomp_hh(Ak)\n",
    "        Ak = r@q\n",
    "    return np.diag(Ak) #, r, q\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b859634d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3p24\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0284a8b-a146-4126-8873-605d96d19a7d",
   "metadata": {},
   "source": [
    "Despite the assumption about disctinct eigenvalues, you can verify the eigenvalues of the matrix studied in week 7 are computed correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f8312-2fbb-45b1-b8a5-fedd242339cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B = np.array([[1, 0.25, 0], [0, 0.5, 0], [0, 0.25, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b319137a-47af-4c29-b074-c26d097851a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eigendecomposition_qr(B, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d12ae36-2ece-4999-a5ec-d9e18781644b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de66b919",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Upload your notebook and separate pdf for theoretical questions if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fac815",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df299f4d",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1p2": {
     "name": "q1p2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> A = np.random.randn(4, 4)\n>>> (Qa, Ra) = qr_decomp_gs(A)\n>>> np.testing.assert_array_almost_equal(Qa @ np.conj(Qa.T), np.eye(4))\n",
         "failure_message": "Q matrix should be orthogonal",
         "hidden": false,
         "locked": false,
         "success_message": "Good, your implementation returns a Q matrix that is orthogonal"
        },
        {
         "code": ">>> A = np.random.randn(5, 5)\n>>> (Qa, Ra) = qr_decomp_gs(A)\n>>> assert np.allclose(Ra, np.triu(Ra))\n",
         "failure_message": "R matrix should be upper triangular",
         "hidden": false,
         "locked": false,
         "success_message": "Good, your implementation returns a R matrix that is upper triangular"
        },
        {
         "code": ">>> A = np.random.randn(7, 7)\n>>> (Qa, Ra) = qr_decomp_gs(A)\n>>> np.testing.assert_array_almost_equal(Qa @ Ra, A)\n",
         "failure_message": "check your implementation",
         "hidden": false,
         "locked": false,
         "success_message": "Good,  the product QR gives the original matrix"
        },
        {
         "code": ">>> from unittest.mock import patch\n>>> \n>>> def check_scipy_qr(N):\n...     with patch('scipy.linalg.qr') as mock_qr:\n...         A = np.random.randn(N, N)\n...         (Qa, Ra) = qr_decomp_gs(A)\n...         mock_qr.assert_not_called()\n>>> check_scipy_qr(8)\n",
         "failure_message": "Do not use scipy.linalg.qr in your implementation",
         "hidden": false,
         "locked": false,
         "success_message": "Good, you did not use the scipy.linalg.qr function"
        },
        {
         "code": ">>> from unittest.mock import patch\n>>> \n>>> def check_numpy_qr(N):\n...     with patch('numpy.linalg.qr') as mock_qr:\n...         A = np.random.randn(N, N)\n...         (Qa, Ra) = qr_decomp_gs(A)\n...         mock_qr.assert_not_called()\n>>> check_numpy_qr(6)\n",
         "failure_message": "Do not use numpy.linalg.qr in your implementation",
         "hidden": false,
         "locked": false,
         "success_message": "Good, you did not use the numpy.linalg.qr function"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1p3": {
     "name": "q1p3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> from unittest.mock import patch\n>>> \n>>> def check_inv_qr():\n...     with patch('numpy.linalg.inv') as mock_qr:\n...         A = np.array([[12, -51, 4], [6, 167, -68], [-4, 24, -41]])\n...         b = np.array([1, -1, 1])\n...         x = solve_gs(A, b)\n...         mock_qr.assert_not_called()\n>>> check_inv_qr()\n",
         "failure_message": "Do not use numpy.linalg.inv in your implementation",
         "hidden": false,
         "locked": false,
         "success_message": "Good, you did not use the numpy.linalg.inv function"
        },
        {
         "code": ">>> A = np.array([[12, -51, 4], [6, 167, -68], [-4, 24, -41]])\n>>> b = np.array([1, -1, 1])\n>>> x = solve_gs(A, b)\n>>> np.testing.assert_array_almost_equal(A @ x, b)\n",
         "failure_message": "Check your implementation",
         "hidden": false,
         "locked": false,
         "success_message": "Good, your results seem correct"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3p24": {
     "name": "q3p24",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> A = np.array([[-2, -4, 2], [-2, 1, 2], [4, 2, 5]])\n>>> v = eigendecomposition_qr(A, 150)\n>>> np.testing.assert_array_almost_equal(v, np.array([6, -5, 3]))\n",
         "failure_message": "Check your implementation",
         "hidden": false,
         "locked": false,
         "success_message": "Good, your results seem correct"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
