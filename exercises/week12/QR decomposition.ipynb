{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833d835",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"QR decomposition.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d71c2-7b02-4185-9a08-5dc10d903dad",
   "metadata": {},
   "source": [
    "# Matrix Analysis 2024 - EE312\n",
    "\n",
    "## Week 12 - QR DECOMPOSITION\n",
    "[LTS2](https://lts2.epfl.ch)\n",
    "\n",
    "### Objectives\n",
    "The goal of this week's exercise session is to study some aspects of the QR decomposition and its connections to projections and inverse. We will study two methods that can be used to compute the QR decomposition of a matrix and an application to eigendecomposition.\n",
    "\n",
    "Please submit your answers **individually**.\n",
    "\n",
    "Let us consider a square $n\\times n$ real matrix $A$ having linearly independent columns (NB: QR factorization is applicable to rectangular matrices, we consider square matrices for simplification). The QR decomposition of $A$ is $A=QR$ where $Q$ is an orthogonal matrix and $R$ and upper-triangular matrix.\n",
    "\n",
    "\n",
    "\n",
    "# 1. Gram-Schmidt orthogonalization\n",
    "\n",
    "Reminder:\n",
    "- The projection operator of a vector $v$ on another $u$ is given by $P_u v = \\frac{\\langle u,v \\rangle}{\\langle u,u \\rangle}u$.\n",
    "- Gram-Schmidt orthogonalization process of a basis $(v_1, v_2, ..., v_{n})$ produces an orthormal basis $(e_1, e_2, ..., e_{n})$ as follows:\n",
    "\n",
    "$u_1 = v_1$, $e_1 = \\frac{u_1}{\\|u_1\\|}$\n",
    "\n",
    "$u_2 = v_2 - P_{u_1}v_2$, $e_2 = \\frac{u_2}{\\|u_2\\|}$\n",
    "\n",
    "$u_3 = v_3 - P_{u_2}v_3 - P_{u_1}v_3$, $e_3 = \\frac{u_3}{\\|u_3\\|}$\n",
    "\n",
    "...\n",
    "\n",
    "$u_k = v_k - \\sum_{j=1}^{k-1} P_{u_j}v_{k}$, $e_k = \\frac{u_k}{\\|u_k\\|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2397ef-938a-46da-a0c9-1a78fa322046",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 1.1 \n",
    "We perform the QR decomposition of $A$ by applying Gram-Schmidt to its column vectors of $A$ (denoted by $a_k, k=1,...n)$. $Q$ is made of the column vectors $e_k$, i.e. $Q=(e_1|e_2|...|e_{n})$. Let us denote by $r_{jk}$ the coefficients of $R$.\n",
    "\n",
    "Prove that $r_{kk} = \\|u_k\\|$, $r_{jk} = <e_j, a_k>$ for $j<k$ and $r_{jk} = 0$ for $j>k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ea06fc",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93abc7b-3142-4515-a03e-a7b582af3815",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 1.2\n",
    "Implement a function that performs the QR decomposition of a square $n\\times n$ matrix using the Gram-Schmidt orthogonalization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b053a5-e135-49c5-8d8e-bcb3a40a1970",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455d1c2-2f2c-4316-81e5-7eb202f87946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def qr_decomp_gs(A):\n",
    "    \"\"\"\n",
    "    Performs the QR decomposition of the matrix using Gram-Schmidt\n",
    "    \n",
    "    Parameters: \n",
    "      - A: input matrix nxn\n",
    "      \n",
    "    Returns:\n",
    "      - Q matrix (orthogonal)\n",
    "      - R matrix (upper triangular)\n",
    "    \"\"\"\n",
    "\n",
    "    n = A.shape[0]\n",
    "    Q = np.zeros((n,n))\n",
    "    R = np.zeros((n,n))\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548f4f3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1p2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3460934-a3b6-4c79-bd61-f1eeac645f73",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 1.3\n",
    "Using the properties of Q, write a routine that can solve a linear system $Ax=b$ using the QR factorization of $A$ ($A$ being a $n\\times n$ real matrix).\n",
    "What property do the coefficients of $R$ need to satisfy for the solution to exist ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00940be7-8e7b-42f9-a691-ea4631a235e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def solve_gs(A, b):\n",
    "    \"\"\"\n",
    "    Solve a linear system Ax=b using QR decomposition\n",
    "    \n",
    "    Parameters:\n",
    "      - A input matrix nxn\n",
    "      - b result vector (length n)\n",
    "    \n",
    "    Returns:\n",
    "      - solution vector x (length n)\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    x = np.zeros(n)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed57911-5f2e-4953-b2ce-26dd669daa84",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Small numerical example\n",
    "A=np.array([[12,-51,4],[6,167,-68],[-4,24,-41]])\n",
    "b=np.array([1, -1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4a98d-cdc8-4c09-914c-0e95fc0483ca",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "x = solve_gs(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797892aa-a084-4524-8f74-545094e5bdcf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "A@x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "488c1a2a-1735-4d3b-9a25-2a496c031dc0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39362e95",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1p3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d1ec0-f35b-4ff0-9b69-feeb56a7bac0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 1.4\n",
    "Consider the matrix \n",
    "$B=\\begin{pmatrix}1 & 1 & 1 \\\\ \\varepsilon & 0 & 0\\\\ 0 & \\varepsilon & 0\\end{pmatrix}$, with $\\varepsilon$ being small enough (typically $10^{-8}$ or smaller). In that case, is the method used to compute Q and R still reliable (and why) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8015bbb0-2d23-4e57-bbb9-3feba8d4e7c8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "eps=1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25058499",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d39aa-1c4a-4d22-bf74-5bd51fcc2563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cdacf1-a8d7-4b80-be17-036fc0263212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the orthogonality of Q\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e866b116-404c-4ab6-8e69-bb7caa5ff46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the validity of the decomposition\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ec7260-2cda-4990-b08a-2cc45e17c103",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# 2. Householder reflections\n",
    "\n",
    "We will now study and implement another method to perform the QR decomposition that is more resistant to the numerical issues mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21765803-4126-4b71-84b9-5c77ac3137cc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 2.1 \n",
    "Let $v$ be a unit vector and the associated transform $H_v=I-2vv^T$. Prove that $H_v$ is orthogonal and that it preserves the norm. What is the effect of $H_v$ on a vector $u$ orthogonal to $v$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254fd21b",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5352bba-8353-443b-aa53-249b0341fdc2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 2.2 \n",
    "What is the effect of $H_v$ on any vector $u$ (drawing the planar case can be of help)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e13357",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b255fc-51a6-4f49-ab27-e00ea87e8394",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 2.3\n",
    "The goal is now to design $n$ reflection operators $H_{v_1}, H_{v_2}, ..., H_{v_n}$ s.t. we have\n",
    "$$\n",
    "H_{v_n}...H_{v_2}H_{v_1}A = R,\n",
    "$$\n",
    "where $R$ is an upper triangular matrix. \n",
    "\n",
    "The reflection operations are meant to be \"progressive\", i.e. $H_{v_k}...H_{v_1}A$ has its $k$ first columns upper triangular.\n",
    "\n",
    "- Find $v_1$ and $\\alpha$ s.t. $H_{v_1}a_1 = \\alpha e_1$, where $a_1$ is the first column of $A$ and $e_1 = \\begin{pmatrix}1\\\\0\\\\ \\vdots \\\\0 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e5ba6-ab4f-42f8-b4ac-5d0ddebef1d1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "- Let us now generalize the previous step to find $H_{v_k}$. Given a vector $x = \\begin{pmatrix}x^U \\\\ x^L\\end{pmatrix}$ with $x^U \\in\\mathbb{R}^k$\n",
    "and $x^L\\in\\mathbb{R}^{n-k}$. \n",
    "\n",
    "Find a vector $v_k$ s.t. $H_{v_k}x = \\begin{pmatrix}x^U \\\\ \\alpha e_1^L \\end{pmatrix}$, where $\\alpha\\in\\mathbb{R}$ and \n",
    "$e_1^L = \\begin{pmatrix}1\\\\0\\\\ \\vdots \\\\0 \\end{pmatrix} \\in \\mathbb{R}^{n-k}$. \n",
    "\n",
    "It might be of help to write $v_k=\\begin{pmatrix}v_k^U\\\\v_k^L\\end{pmatrix}$, with $v_k^U \\in\\mathbb{R}^k$\n",
    "and $v_k^L\\in\\mathbb{R}^{n-k}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d2bae",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c386e-e498-4b85-ac8a-f3bea7230c6e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 2.4\n",
    "We now have all we need to compute the $H_{v_k}$ matrices. Implement the QR decomposition using the above results. \n",
    "You should have found that the sign of $\\alpha$ can be either positive or negative. Choose $\\alpha$ to have the sign opposite to the one of $x^L_k[0]$. (Use the `numpy.sign`function).\n",
    "\n",
    "Try your implementation on the $B$ matrix seen in ex. 1. What is the advantage of this implementation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13bdb8-5046-42e4-a3e0-f58654acce81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def qr_decomp_hh(A):\n",
    "    \"\"\"\n",
    "    Performs the QR decomposition of the matrix using Householder reflections\n",
    "    \n",
    "    Parameters: \n",
    "      - A: input matrix nxn\n",
    "      \n",
    "    Returns:\n",
    "      - Q matrix (orthogonal)\n",
    "      - R matrix (upper triangular)\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc670d-b80e-4ad0-bf4a-54ec5bc9aa8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qh, rh = qr_decomp_hh(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad58b8-9d20-4a25-81b7-ff8c5684d0b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check orthogonality of Q\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211740b-24f4-4408-8bb9-7d83ca0f46f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check validity of solution\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fafb41-cc9d-4936-bbac-18b60797f66d",
   "metadata": {},
   "source": [
    "# 3. Eigendecomposition and QR\n",
    "\n",
    "We will now study some aspects of how to actually perform (numerically) the eigendecomposition of a matrix. \n",
    "\n",
    "In the following we will suppose that $A$ has distinct eigenvalues, and we will write them as $\\lambda_1, ..., \\lambda_n$ s.t. $|\\lambda_1| > |\\lambda_2|...>|\\lambda_n|$, and denote their associated normalized eigenvectors as $v_1, v_2..., v_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05429ad4-8b35-47d5-ad98-3ccc8300a658",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 3.1 Power method\n",
    "\n",
    "Let us consider a vector $w\\in\\mathbb{R}^n$ of norm 1, and its associated eigendecomposition $w = \\sum_{k=1}^n\\alpha_k v_k$. \n",
    "\n",
    "Compute $A^pw$ (for $p\\in\\mathbb{N}$, $k>0$). Assuming that $\\alpha_1\\neq 0$ for the chosen $w$, use this result to find an algorithm to compute $\\lambda_1$ and $v_1$ (you may want to introduce another vector $x\\in\\mathbb{R}^n$ s.t. $x^Tv_1\\neq 0$ and consider the quantity $x^TA^pw$) ?\n",
    "\n",
    "How would you proceed for $\\lambda_2$ and $v_2$ and the other eigenvalues ?\n",
    "\n",
    "What are the limitations of this method ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e45de",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405a28b-fcae-48c6-89df-cb0ac25f9750",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 3.2 QR method\n",
    "\n",
    "### 3.2.1\n",
    "Prove that the matrix $B_1=(I-v_1v_1^T)A$ has eigenvalues $0, \\lambda_2, ..., \\lambda_n$, with corresponding eigenvectors $v_1, (I-v_1v_1^T)v_2, ..., (I-v_1v_1^T)v_n$. How about the matrices $B_2=(I-v_2v_2^T)(I-v_1v_1^T)A, B_3=(I-v_3v_3^T)(I-v_2v_2^T)(I-v_1v_1^T)A$, etc. when supposing that $v_1, v_2...$ are orthogonal ?\n",
    "\n",
    "Using question 1 (you can show the operator $I-v^Tv$ is indeed the basis operation of Gram-Schmidt), reformulate the following algorithm using the QR decomposition of $AV^{(k)}$ (denoted by $V^{(k+1)}R^{(k+1)}$):\n",
    "- Start with $v_1^{(0)}, v_2^{(0)}, ..., v_n^{(0)}=V^{(0)}$\n",
    "- Iterate for $k=0, 1, ... $\n",
    "  - $v_1^{(k+1)}=Av_1^{(k)}$ and normalize $v_1^{(k+1)}$\n",
    "  - $v_2^{(k+1)} = (I-v_1^{(k+1)}v_1^{(k+1)T})Av_1^{(k)}$ and normalize $v_2^{(k+1)}$\n",
    "  - ...\n",
    "  - $v_n^{(k+1)} = (I-v_{n-1}^{(k+1)}v_{n-1}^{(k+1)T})...(I-v_1^{(k+1)}v_1^{(k+1)T})Av_1^{(k)}$ and normalize $v_n^{(k+1)}$\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d895858",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2168af1e-835a-45d0-b9ac-50eb763eec20",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.2.2\n",
    "\n",
    "How does the algorithm of question 3.2.1 relates to the one from question 3.1 ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2eafe7",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef48172f-8fe5-4501-94e9-22dc9ee85999",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.2.3\n",
    "\n",
    "Let us introduce the matrix $A^{(k)}=V^{(k)T}AV^{(k)}$, where $(v_1^{(k)}, v_2^{(k)}, ..., v_n^{(k)})=V^{(k)}$. Let us also introduce the QR decomposition of $A^{(k)}=Q^{(k+1)}R^{(k+1)}$.\n",
    "\n",
    "Prove that $A^{(k+1)} = R^{(k+1)}Q^{(k+1)}$. What is the interest of this sequence of matrices $A^{(k)}$ regarding the eigenvalues of $A$ ?\n",
    "\n",
    "Hint: remember the QR decomposition is unique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5fc2a3",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729e9855-33aa-452f-a3ab-57258120933d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 3.2.4\n",
    "\n",
    "Implement the algorithm that generates the sequence $A^{(k)}$. The diagonal elements of $A^{(k)}$ will converge towards the eigenvalues of $A$ (you do not need to prove this result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219bc1f2-0003-4b23-ab02-e665d7b81568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eigendecomposition_qr(A, num_iter=20):\n",
    "    \"\"\"\n",
    "    Computes the eigenvalues of A using the QR iteration method\n",
    "    \n",
    "    Parameters:\n",
    "      - A: input nxn matrix\n",
    "      - num_iter: number of QR iterations to perform\n",
    "      \n",
    "    Returns:\n",
    "      - a vector containing the eigenvalues\n",
    "    \"\"\"\n",
    "    Ak = A\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b859634d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3p24\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0284a8b-a146-4126-8873-605d96d19a7d",
   "metadata": {},
   "source": [
    "Despite the assumption about disctinct eigenvalues, you can verify the eigenvalues of the matrix studied in week 7 are computed correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f8312-2fbb-45b1-b8a5-fedd242339cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B = np.array([[1, 0.25, 0], [0, 0.5, 0], [0, 0.25, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b319137a-47af-4c29-b074-c26d097851a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eigendecomposition_qr(B, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d12ae36-2ece-4999-a5ec-d9e18781644b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de66b919",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Upload your notebook and separate pdf for theoretical questions if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fac815",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df299f4d",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1p2": {
     "name": "q1p2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> A = np.random.randn(4, 4)\n>>> (Qa, Ra) = qr_decomp_gs(A)\n>>> np.testing.assert_array_almost_equal(Qa @ np.conj(Qa.T), np.eye(4))\n",
         "failure_message": "Q matrix should be orthogonal",
         "hidden": false,
         "locked": false,
         "success_message": "Good, your implementation returns a Q matrix that is orthogonal"
        },
        {
         "code": ">>> A = np.random.randn(5, 5)\n>>> (Qa, Ra) = qr_decomp_gs(A)\n>>> assert np.allclose(Ra, np.triu(Ra))\n",
         "failure_message": "R matrix should be upper triangular",
         "hidden": false,
         "locked": false,
         "success_message": "Good, your implementation returns a R matrix that is upper triangular"
        },
        {
         "code": ">>> A = np.random.randn(7, 7)\n>>> (Qa, Ra) = qr_decomp_gs(A)\n>>> np.testing.assert_array_almost_equal(Qa @ Ra, A)\n",
         "failure_message": "check your implementation",
         "hidden": false,
         "locked": false,
         "success_message": "Good,  the product QR gives the original matrix"
        },
        {
         "code": ">>> from unittest.mock import patch\n>>> \n>>> def check_scipy_qr(N):\n...     with patch('scipy.linalg.qr') as mock_qr:\n...         A = np.random.randn(N, N)\n...         (Qa, Ra) = qr_decomp_gs(A)\n...         mock_qr.assert_not_called()\n>>> check_scipy_qr(8)\n",
         "failure_message": "Do not use scipy.linalg.qr in your implementation",
         "hidden": false,
         "locked": false,
         "success_message": "Good, you did not use the scipy.linalg.qr function"
        },
        {
         "code": ">>> from unittest.mock import patch\n>>> \n>>> def check_numpy_qr(N):\n...     with patch('numpy.linalg.qr') as mock_qr:\n...         A = np.random.randn(N, N)\n...         (Qa, Ra) = qr_decomp_gs(A)\n...         mock_qr.assert_not_called()\n>>> check_numpy_qr(6)\n",
         "failure_message": "Do not use numpy.linalg.qr in your implementation",
         "hidden": false,
         "locked": false,
         "success_message": "Good, you did not use the numpy.linalg.qr function"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1p3": {
     "name": "q1p3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> from unittest.mock import patch\n>>> \n>>> def check_inv_qr():\n...     with patch('numpy.linalg.inv') as mock_qr:\n...         A = np.array([[12, -51, 4], [6, 167, -68], [-4, 24, -41]])\n...         b = np.array([1, -1, 1])\n...         x = solve_gs(A, b)\n...         mock_qr.assert_not_called()\n>>> check_inv_qr()\n",
         "failure_message": "Do not use numpy.linalg.inv in your implementation",
         "hidden": false,
         "locked": false,
         "success_message": "Good, you did not use the numpy.linalg.inv function"
        },
        {
         "code": ">>> A = np.array([[12, -51, 4], [6, 167, -68], [-4, 24, -41]])\n>>> b = np.array([1, -1, 1])\n>>> x = solve_gs(A, b)\n>>> np.testing.assert_array_almost_equal(A @ x, b)\n",
         "failure_message": "Check your implementation",
         "hidden": false,
         "locked": false,
         "success_message": "Good, your results seem correct"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3p24": {
     "name": "q3p24",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> A = np.array([[-2, -4, 2], [-2, 1, 2], [4, 2, 5]])\n>>> v = eigendecomposition_qr(A, 150)\n>>> np.testing.assert_array_almost_equal(v, np.array([6, -5, 3]))\n",
         "failure_message": "Check your implementation",
         "hidden": false,
         "locked": false,
         "success_message": "Good, your results seem correct"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
