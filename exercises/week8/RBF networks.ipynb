{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa451194",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"RBF networks.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62d1e09-41c3-40d0-a763-e8231d437183",
   "metadata": {},
   "source": [
    "# Matrix Analysis 2024 - EE312\n",
    "## Week 8 - Image classification with Radial Basis Function (RBF) networks\n",
    "[LTS2](https://lts2.epfl.ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.special import softmax\n",
    "import requests\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-basin",
   "metadata": {},
   "source": [
    "##  1. Image Classification\n",
    "In this exercise, we will be doing image classification with a simple neural network. For simplicity, let's assume we will be working with black and white images.\n",
    "Given an input image $i$ represented as a vector of  pixel intensities $ \\mathbf{x}_i \\in [0,1]^d$, we want to predict its correct label $\\mathbf{y}_i$, which is represented as a one-hot vector in $\\{0,1\\}^K$, where $K$ is the number of possible categories (classes) that the image may belong to. For example, we may have pictures of cats and dogs, and our goal would be to correctly tag those images as either cat or dog. In that case we would have $K=2$, and the vectors $\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$ and $\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ to represent the classes of cat and dog.  \n",
    "\n",
    "In today's example we will be using the MNIST handwritten digit dataset. It contains images of handwritten numbers from 0 to 9 and our goal is to create a model that can accurately tag each image with its number. Let's load the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the data\n",
    "\n",
    "# Download data if needed\n",
    "if not Path(\"./mnist_data.npz\").is_file():\n",
    "    r = requests.get('https://os.unil.cloud.switch.ch/swift/v1/lts2-ee312/mnist_data.npz', allow_redirects=True)\n",
    "    with open('mnist_data.npz', 'wb') as f: # save locally\n",
    "        f.write(r.content)\n",
    "\n",
    "\n",
    "mnist = np.load('mnist_data.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-taylor",
   "metadata": {},
   "source": [
    "In the context of classification, neural networks are models that given one (or multiple) input data points produce as output a set of corresponding labels for each input. The model itself consists of parametric functions $g_i$ which can be applied sequentially to the input data, resulting in a set of labels which are the model's prediction for the data. For example, in a model that consists of two parameteric functions $g_1$ and $g_2$, for a given $\\mathbf{x}_i$, we have the predicted label $ \\hat{\\mathbf{y}}_i = g_1(g_2(\\mathbf{x}_i))$. The parametric functions are commonly called \"layers\".\n",
    "\n",
    "In a standard image classification setup, we are given some training data which we can use to tune the parameters of the parametric functions $g_i$ in order to improve its ability to predict the labels correctly. The parameters are generally tuned with respect to some objective (commonly called a loss function). We want to find the parameters of the model that minimize this loss function. Various loss functions can be used, but in general they tend to encode how \"wrong\" the model is. For\n",
    "example, on a given image $i$ one can use the loss $\\mathcal{L}(\\hat{\\mathbf{y}_i}, \\mathbf{y}_i)= \\sum_{j=1}^{K}(\\hat{{y}}_{ij} -{y}_{ij})^2 $, which is the mean squared difference between the vector coordinates of the predicted label of the image and the ones of the actual label $\\mathbf{y}_i$.\n",
    "Minimizing the loss over the whole training set is referred to as \"training the model\". Furthermore, the goal is that given new data we have not seen before and we have not trained our model with, the model will still be able to classify accurately.\n",
    "\n",
    "Before we go into the details of the model and how we will train it, let's prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2140162-4414-47c2-ae99-d616300a1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "images = mnist['data']\n",
    "num_images = images.shape[0]\n",
    "\n",
    "train_set_size = 60000\n",
    "test_set_size = 10000\n",
    "\n",
    "train_images = images[:train_set_size]\n",
    "train_images = train_images/255.\n",
    "train_images =  train_images\n",
    "\n",
    "test_images = images[-test_set_size:]\n",
    "test_images = test_images/255.\n",
    "test_images = test_images\n",
    "\n",
    "#create one-hot encodings of labels\n",
    "mnist_target = mnist['target']\n",
    "num_classes = mnist_target.max()+1\n",
    "labels = []\n",
    "for k in range(num_images):\n",
    "    one_hot = np.zeros(num_classes)\n",
    "    one_hot[int(mnist_target[k])]=1\n",
    "    labels += [one_hot]\n",
    "labels = np.array(labels)\n",
    "\n",
    "#labels in one-hot\n",
    "train_labels = labels[:train_set_size]\n",
    "test_labels = labels[-test_set_size:]\n",
    "\n",
    "#labels in integer form\n",
    "int_labels = np.array(mnist_target, dtype=int)\n",
    "int_labels_train = int_labels[:train_set_size]\n",
    "int_labels_test = int_labels[-test_set_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b3e353-a254-4cb5-975d-0741a50576c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View an image to make sure everything went well\n",
    "which_one = 5\n",
    "plt.imshow(train_images[which_one].reshape((28,28)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-tower",
   "metadata": {},
   "source": [
    "## 2. Radial Basis Function (RBF) networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-malaysia",
   "metadata": {},
   "source": [
    "For our task, we will be using Radial Basis Function (RBF) Networks as our neural network model.\n",
    "The pipeline, which is presented in the image below, consists of two layers. The first employs non-linear functions $g_1(\\mathbf{x};\\boldsymbol{\\mu}): \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times c}$.\n",
    "The second is a linear layer, represented by a matrix of weights $\\mathbf{W} \\in \\mathbb{R}^{c \\times K}$, which maps the output of the previous layer to class scores; its role is to predict labels. \n",
    "\n",
    "The pipeline proceeds in the following steps:\n",
    "\n",
    "i) Choose a set of $c$ points $\\boldsymbol{\\mu}_j\\in [0,1]^d$.     \n",
    "ii) Compute $g_1(\\mathbf{x}_i;\\boldsymbol{\\mu}_j) = \\exp^{-\\frac{||{\\mathbf{x}_i-\\boldsymbol{\\mu}_j||^2}}{\\sigma^2}}=a_{ij}$ for all possible pairs of $i$ and $j$. Here $\\sigma$ is a hyperparameter that controls the width of the gaussian.  \n",
    "iii) Compute the predicted labels $g_2(\\mathbf{a}_i)= \\mathbf{a}_i^{\\top}\\mathbf{W}= \\hat{\\mathbf{y}}_i$. Here $\\mathbf{a}_i \\in \\mathbb{R}^c$ are the outputs of the layer $g_1$ for an input image $i$. $\\hat{\\mathbf{y}}_i$ is a row vector and $\\hat{y}_{ij} = \\sum_{m=1}^{c}a_{im}w_{mj}$, $j\\in\\{1,...,K\\}$. \n",
    "\n",
    "![RBF_NN.png](images/RBF_NN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f762f16e-4a88-43a8-813f-c93c62407c72",
   "metadata": {},
   "source": [
    "Intuitively, the first layer of the RBF network can be viewed as matching the input data with a set of prototypes (templates) through a gaussian whose width is determined by $\\sigma$. The second layer performs a weighted combination of the matching scores of the previous layer to determine the predicted label for a given point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126fdb8e-9607-4f93-9726-692e5ed8bb91",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.** For hyperparameters $c$ and $\\sigma$ of your choice, select $c$ prototypes and obtain the output of the first layer of the RBF network. The prototypes can simply be random images from your training set.\n",
    "\n",
    "The following functions might be helpful:\n",
    "- [pairwise_distances (from scikit-learn)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html)\n",
    "- [random.choice (from numpy)](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)\n",
    "\n",
    "You can (optionally) perform an additional normalization step on the activations using the [softmax](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html) function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ab127-352e-49c3-9b72-9e69bfc8b4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rand_centers(num_centers, imgs):\n",
    "    \"\"\"\n",
    "    Sample num_centers (randomly) from imgs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_centers : number of samples\n",
    "    imgs : matrix to sample rows from\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The samples matrix\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "def get_activations(imgs, rand_centers, sigma, softmax_norm=False):\n",
    "    \"\"\"\n",
    "    Computes the activations of the images vs. the sample centers\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imgs: images matrix to compute activations for\n",
    "    rand_centers: matrix of centers points\n",
    "    sigma: parameter of the gaussian kernel\n",
    "    softmax_norm: if true, perform softmax activation on the activations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The activation matrix A\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e1cca2-bb2c-40c7-b64a-e14c3b42e54d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pick random centers\n",
    "number_of_centers = 200\n",
    "\n",
    "rand_centers = get_rand_centers(number_of_centers, train_images)\n",
    "\n",
    "sigma = 10\n",
    "activations = get_activations(train_images, rand_centers, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346dd3dd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-thompson",
   "metadata": {},
   "source": [
    "## 3. Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-present",
   "metadata": {},
   "source": [
    "To make things easier, we will fix the parameters $\\boldsymbol{\\mu}$ and $\\sigma$ of the network, i.e., we decide their values before and the remain constant throughout training and testing of the model. Therefore, the only trainable parameters are going to be the weights of the second layer.\n",
    "To train the model, we are going to use the mean squared loss function that we mentioned earlier. For a training dataset with $n$ images we have\n",
    "\n",
    "$$ \\mathcal{L}(\\text{training data}, \\text{training labels}) = \\frac{1}{2n}\\sum_{i=1}^n\\mathcal{L}(\\hat{\\mathbf{y}}_i,\\mathbf{y}_i) = \\frac{1}{2n}\\sum_{i=1}^n ||(\\hat{\\mathbf{y}}_{i} - \\mathbf{y}_{i})||^2.$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " There are two ways of tuning those:  \n",
    "i) Backpropagation.   \n",
    "ii) Solve a linear system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef20c5-0cc0-4a7d-a80b-8acfa207f82e",
   "metadata": {},
   "source": [
    "### 3.1 Training with backpropagation\n",
    "\n",
    "Backpropagation depends on [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent#Description). The goal is to update the trainable parameters of the network by \"moving them\" in the direction that will decrease the loss function.\n",
    "In our case, the weights $w_{kl}$ are updated in the following manner\n",
    "$$ w_{kl}' = w_{kl}- \\gamma \\frac{\\partial\\mathcal{L}(\\text{training data}, \\text{training labels})}{\\partial w_{kl}}, $$\n",
    "where $\\gamma$ is a hyper-parameter called the learning rate. The gradient of the Loss points towards the direction of steepest descent, hence we update the weights of the network towards that direction.  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e8b8e-a909-4f9e-950a-e0f65a48d70c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "2. For the mean squared error loss, what is the gradient of the loss with respect to the weights $w_{kl}$ of the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34320af7",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-mother",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "3. Train the weights of the linear layer using stochastic gradient descent. For $p$ iterations (called epochs), you have to update each weight $w_{kl}$ of the network once for each image, by computing the gradient of the loss with respect to that weight.\n",
    "\n",
    "NB: if you implement gradient computation naively, it might be very slow. Consider using [numpy.outer](https://numpy.org/doc/stable/reference/generated/numpy.outer.html) to speed up computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1a6cc-c9d5-4561-8c0d-0f0c5b396ae1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function to compute the loss\n",
    "def get_predictions_loss(activations, weight, labels, int_labels):\n",
    "    predictions = activations@weights\n",
    "    num_correct_predictions = ((predictions.argmax(1) - int_labels)==0).sum()\n",
    "    loss = ((predictions - labels)*(predictions - labels)).sum(1).mean()\n",
    "    return loss, num_correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e365cb99-d2c3-4b98-a814-e5c5485f3967",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute the gradient for a single input\n",
    "def compute_gradient(activation, weights, train_label):\n",
    "    \"\"\"\n",
    "    Computes gradients of the weight for a single activation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    activation : vector containing the activation of the current image\n",
    "    weights: current weights\n",
    "    train_label: label of the current image\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The gradient to update the weights \n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e31c54-3682-4124-8a08-f675fba974f7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial values for hyperparams. Feel free to experiment with them.\n",
    "weights = (1/28)*np.random.randn(number_of_centers, num_classes)\n",
    "epochs = 5 # you should train for more epochs !\n",
    "learning_rate = 0.1\n",
    "\n",
    "#Backpropagation with SGD\n",
    "for k in range(epochs):\n",
    "    for counter, activation in enumerate(activations):\n",
    "        gradient = ...\n",
    "        weights = ...\n",
    "    \n",
    "    loss_train, num_correct_predictions_train = get_predictions_loss(activations, weights, train_labels, int_labels_train)\n",
    "    print(\"Loss:\", loss_train)\n",
    "    print(\"Number of correct predictions:\", num_correct_predictions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4390132b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-imperial",
   "metadata": {},
   "source": [
    "We can now check how well your network does on the test set and print its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predictions, int_labels, set_size):\n",
    "    num_correct_predictions = ((predictions.argmax(1) - int_labels)==0).sum()\n",
    "    return num_correct_predictions/set_size\n",
    "\n",
    "test_activations = get_activations(test_images, rand_centers, sigma)\n",
    "test_predictions = test_activations@weights\n",
    "print(f\"The accuracy on the test set is: {get_accuracy(test_predictions, int_labels_test, test_set_size)*100} %\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-funds",
   "metadata": {},
   "source": [
    "### 3.2 Solving the linear system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8234141-0eeb-49da-8d37-892a6416e41d",
   "metadata": {},
   "source": [
    "Since we only have one weight matrix to tune, we can avoid learning with backpropagation entirely. Consider the mean squared error for the whole dataset and a one-dimensional binary label $y_i$ for each data point for simplicity. The mean squared loss for the dataset is\n",
    "$$  \\sum_{i=1}^n (\\hat{{y}}_{i} - {y}_{i})^2=  ||(\\mathbf{A}\\mathbf{w} - \\mathbf{y})||^2.$$ Here $\\mathbf{A} \\in \\mathbb{R}^{n \\times c}$ is the matrix that contains the outputs (activations) of the first layer. From a linear algebra perspective, we are looking for a matrix $\\mathbf{w}$ that solves the linear system $ \\mathbf{A}\\mathbf{w} = \\mathbf{y}.$  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e691348-afe4-4021-a42b-3985a4e9999e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "4. Can we find solutions to this system (justify) and how ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d39b17",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-exercise",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "5. Based on your answer above, compute the weights of the neural network that best classify the data points of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b80d08",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-renaissance",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate the weights of the linear layer\n",
    "weights_lsq = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-foster",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#predict the labels of each image in the training set and compute the accuracy\n",
    "train_prediction_lsq = ...\n",
    "print(f\"The accuracy on the training set is: {get_accuracy(train_prediction_lsq, int_labels_train, train_set_size)*100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-invitation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate the activations of the test set\n",
    "test_activations = get_activations(test_images, rand_centers, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-dover",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#predict the accuracy on the test set\n",
    "test_predictions_lsq = ...\n",
    "print(f\"The accuracy on the test set is: {get_accuracy(test_predictions_lsq, int_labels_test, test_set_size)*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-warrant",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 6. **Open ended**: On the choice of templates. \n",
    "Suggest a different or more refined way to select templates for the RBF network and implement it. Check how it compares with your original approach.\n",
    "Check how it works with the backpropagation and linear system solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea722317",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e7011",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51d702c97c98eba6c840e0251c2afd2cba9486e0f46fe64ab3cf287f49537506"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> imgs = np.ones((100, 20))\n>>> assert get_rand_centers(10, imgs).shape == (10, imgs.shape[1])\n",
         "failure_message": "get_rand_centers should return a (num_centers, img_size) matrix",
         "hidden": false,
         "locked": false,
         "success_message": "Good, your implementation of get_rand_centers returns a matrix of the correct size"
        },
        {
         "code": ">>> imgs = np.outer(np.arange(0, 4), np.ones((1, 10)))\n>>> centers = np.array([imgs[0, :], imgs[3, :]])\n>>> np.testing.assert_almost_equal(get_activations(imgs, centers, 10.0, softmax_norm=False), np.array([[1.0, 0.40656966], [0.90483742, 0.67032005], [0.67032005, 0.90483742], [0.40656966, 1.0]]))\n",
         "failure_message": "Your activations (without softmax) look incorrect, check your implementation",
         "hidden": false,
         "locked": false,
         "success_message": "Good, your activations (without softmax) look correct"
        },
        {
         "code": ">>> imgs = np.outer(np.arange(0, 4), np.ones((1, 10)))\n>>> centers = np.array([imgs[0, :], imgs[3, :]])\n>>> np.testing.assert_almost_equal(get_activations(imgs, centers, 10.0, softmax_norm=True), np.array([[0.64415184, 0.35584816], [0.5583621, 0.4416379], [0.4416379, 0.5583621], [0.35584816, 0.64415184]]))\n",
         "failure_message": "Your activations (with soiftmax) look incorrect, check your implementation",
         "hidden": false,
         "locked": false,
         "success_message": "Good, your activations (with softmax) look correct"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> def test_grad(N, Nc, acts, w, label, k):\n...     g = compute_gradient(acts[k, :], w, label)\n...     assert g.shape == (N, Nc)\n...     gt = np.zeros((N, Nc))\n...     gt[k, :] = np.ones((1, Nc))\n...     np.testing.assert_array_almost_equal(g, gt)\n>>> N = 10\n>>> Nc = 2\n>>> act = np.eye(N)\n>>> w = np.ones((N, Nc))\n>>> train_label = 0\n>>> for k in range(N):\n...     test_grad(N, Nc, act, w, train_label, k)\n",
         "failure_message": "Check your gradient computation",
         "hidden": false,
         "locked": false,
         "success_message": "Good, your gradient look correct"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
