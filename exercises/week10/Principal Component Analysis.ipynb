{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82667bf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"Principal Component Analysis.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1c814-e75e-45bb-bda0-46fc1706459d",
   "metadata": {},
   "source": [
    "# Matrix Analysis 2024 - EE312\n",
    "## Week 10 - Principal Component Analysis (PCA)\n",
    "[LTS2](https://lts2.epfl.ch)\n",
    "\n",
    "PCA is a classic technique for dimensionality reduction. As you will see in this notebook, this method uses  projections and eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777f8f8c-3aaa-4441-bdd5-e933a924aedd",
   "metadata": {},
   "source": [
    "## 1. Eigenvalues and PCA\n",
    "\n",
    "Let us consider $N$ data points $\\{x_k\\}, k=1, ..., N$ in $\\mathbb{R}^d$. During the rest of the exercise we will make the assumption that the mean value of these data points is 0, i.e. $\\frac{1}{N}\\sum_{k=1}^Nx_k=0$. We will denote by $X$ the $N\\times d$ matrix s.t. :\n",
    "\n",
    "$\n",
    "X = \\begin{pmatrix}\n",
    "x_1^T\\\\\n",
    "x_2^T\\\\ \\vdots \\\\ x_N^T \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a8a794-c0e5-4a3e-8948-1ec3032dcd73",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**1.1**\n",
    "Write the projection of the data points $x_k$ on a unit-norm vector $u\\in\\mathbb{R}^d$ using a matrix operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf6911",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5006b2af",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb2244-00c4-435e-ac7e-13efc48743ba",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**1.3** Let us define the matrix $C = \\frac{1}{N}X^TX$ (referred to as the **sample covariance matrix** in the litterature). What are the properties of this matrix ? What is the implication on its eigenvalues ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f280b7",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8368fa14-0308-46e7-ba2a-f8d21d56bcf1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**1.4** PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on. The following picture illustrate the principle for (blue) points in $\\mathbb{R}^2$. Intuitively, the variance of the projected points (in red) will be maximized when the direction of projection matches the main direction of the point cloud in the picture.\n",
    "\n",
    "![PCA](images/pca.gif)\n",
    "\n",
    "We will now try to find a vector $u$, $||u||=1$, s.t. the variance of the projection of the data on this vector is maximal. Let us order the eigenvalues of $C$ in a decreasing order, i.e. $\\lambda_1\\geq \\lambda_2\\geq...\\geq \\lambda_d$. Show that the eigenvector associated with the largest eigenvalue maximizes the variance of the projection of $x_k$. This will be the first vector used for the first principal component. (Hint: consider the orthonormal basis formed by the eigenvectors of $C$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e15bfa",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d5f24-b360-4db5-999f-c0b924a75199",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**1.5** What is the second vector that will maximize the variance of the $x_k$ minus their projection on the first principal component vector (i.e. eigenvector associated to $\\lambda_1$) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a0c64d",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a9f74-c97c-44b6-bcb0-c341c8aabd36",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 2. Applying PCA to data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2766e4-02db-4ff7-81ff-971e450b21ed",
   "metadata": {},
   "source": [
    "After completing the previous part, you probably figured that PCA is achieved by iterating projections of residuals on eigenvectors of the covariance matrix. In this exercise, you will apply the PCA to a specific dataset.\n",
    "\n",
    "**Warning** Computing the eigenvectors and eigenvalues can take time (> 1 minute) and is too slow on noto. Use either a local installation or [google colab](https://colab.research.google.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d4c065-a837-45c6-93b3-34a40ea27140",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.1 Loading the dataset\n",
    "The [\"Olivetti faces dataset\"](https://scikit-learn.org/stable/datasets/real_world.html?highlight=olivetti) is made of 400 64x64 images (represented as vectors containing 4096 elements). The dataset is made of pictures of 40 persons, with varying light conditions, facial expressions etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaaffb2-9bd3-44f7-87ef-1a8e636889a0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea92d6-5f26-4ae3-9f42-d5c1de786603",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "faces, labels = fetch_olivetti_faces(return_X_y=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b71c6-cc58-4639-8ffc-9f7b78420427",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(faces[2,:], (64,64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d74c26-1b9e-4c55-8586-5f68334449d8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The first step to perform the PCA is to remove the mean value from the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c98aee1-9127-4828-8058-a2c9d997bd3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_mean(input_data):\n",
    "    \"\"\"\n",
    "    Compute the mean vector of the dataset and subtract it\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : input data matrix\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    0-centered input data\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80647d21-857c-42c5-a228-6955de2e3481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "faces_zero_centered = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7087f84",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2p1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88d188a-170e-4755-a3fc-c2c3ef1342d1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.2 Covariance matrix\n",
    "As seen in the theoretical part, you need compute the covariance matrix $C$ and compute its eigenvalues and eigenvectors (use the [numpy.linalg.eigh](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigh.html) function). Using the [argsort](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html) function, sort the eigenvalues and eigenvectors appropriately (do not trust Numpy's `eigh` documentation !). Be careful, `argsort` sorts in ascending order only, do not forget to reverse the array !\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebfdcff-bba0-4f00-a719-635917f3f866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def covariance_sorted_eigv(X):\n",
    "    \"\"\"\n",
    "    Computes the covariance matrix and return its sorted eigenvectors/eigenvalues in decreasing order\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : zero-centered input data\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    The sorted eigenvectors and eigenvalues of the covariance matrix of X \n",
    "    \"\"\"\n",
    "    ...\n",
    "    return sorted_eigen_vals, sorted_eigen_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29138b83-aae6-4b6a-b221-e2f06d5f5b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_eigen_vals, sorted_eigen_vecs = covariance_sorted_eigv(faces_zero_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b90b1-0483-4aa0-9b39-b91716b69152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the eigenvectors associated with the largest eigenvalues\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8725666c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2p2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a127c1-c26c-424d-a6b6-3bf024494653",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.3 PCA \n",
    "We are finally ready to write a function that performs a PCA, i.e. given an input data vector, returns its projection on the $n$ largest eigenvectors (implement the `pca` function). For a given input image, compute an approximation using $n$ principal components (in the `pca_approx` function). How many components (approximately) do you need to have a result that is close to the original image ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc7b51-f001-4ddb-ba90-8a4e4d4791dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pca(input_vec, sorted_eig_vectors, n):\n",
    "    \"\"\"\n",
    "    Compute the projection on the n largest eigenvectors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_vec : input data vector\n",
    "    sorted_eig_vectors: sorted eigenvectors of the covariance matrix\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    The projection of input_vec on each eigen vector\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d31a5b-2ba0-4b1d-8169-7b2179ad59b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pca_approx(pca_projection, sorted_eig_vectors, n):\n",
    "    \"\"\"\n",
    "    Compute the PCA approximation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pca_projection : projection of input data vector on the covariance matrix egigenvectors\n",
    "    sorted_eig_vectors: sorted eigenvectors of the covariance matrix\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    The projection of input_vec on each eigenv vector\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e546ea2-d1f6-4b5c-9caf-e54f9822cf53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 10 # update this number\n",
    "img = faces_zero_centered[40, :] # try other images too\n",
    "approx = ...\n",
    "ax = plt.subplot(121)\n",
    "plt.imshow(np.reshape(img, (64, 64)))\n",
    "ax.set_title('Original')\n",
    "ax = plt.subplot(122)\n",
    "plt.imshow(np.reshape(approx, (64, 64)))\n",
    "ax.set_title('PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5ffb8-d775-4779-8901-4a5b4017049a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Depending on the image, 100 to 200 components yield a good approximation of the initial image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4304ef4-47a4-415f-aece-f3931eeba27f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## 3. PCA and SVD\n",
    "\n",
    "Let us now study the relationship between PCA and SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3a26e-f3f9-4ce9-8e50-f4f350f790e2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.1 \n",
    "Using the SVD of $X$, find a relationship between the eigenvalues/eigenvectors of $C$ and the singular values/singular vectors of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdef60f",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b202ba81-ad47-4112-ae5c-9425b268f735",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.2\n",
    "Check the [svd](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html) function in Numpy and use it to compute the eigenvalues and eigenvectors of $C$ (use `full_matrices=False` to speed up computations). What is the interest of using SVD vs. computing explicitely $C$ and its eigenvalues/eigenvectors ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ae65a8",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8fb9a8-b206-4578-8e5b-832154315dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def covariance_sorted_eigv_svd(X):\n",
    "    \"\"\"\n",
    "    Computes the covariance matrix and return its sorted eigenvectors/eigenvalues in decreasing order using SVD\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : zero-centered input data\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    The sorted eigenvectors and eigenvalues of the covariance matrix of X \n",
    "    \"\"\"\n",
    "    ...\n",
    "    return sorted_eigen_vals, sorted_eigen_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f03290-089e-4a1f-a0f5-cfbe5f9ab2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svd_eigen_vals, svd_eigen_vec = covariance_sorted_eigv_svd(faces_zero_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9628e5-d009-4c7f-928f-ca82f4c62f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b2e49-9573-4af1-8727-d8bf76d55f03",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.3\n",
    "Compute a low-rank $k\\ll d$ approximation $X_k$ of $X$. You can plot the singular values to choose $k$. What is the relationship between $X_k$ and the PCA ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00822f",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d322a798-13b0-4641-b7e1-eaf937469bac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "u,s,vh = np.linalg.svd(faces_zero_centered, full_matrices=False)\n",
    "plt.plot(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a0fbcd-b34d-4ab9-82f8-2d8ecbf50062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def approx_low_rank(X, k):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a998dba5-b679-400b-b25f-53e8f3d5928f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k= ...\n",
    "X_k = approx_low_rank(faces_zero_centered, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45643bea",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd81ef7-5d7e-4bde-94af-35064c6ee986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot low-rank approximation\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "for k in range(nrows*ncols):\n",
    "    plt.subplot(nrows, ncols, k+1)\n",
    "    plt.imshow(np.reshape(X_k[k, :], (64, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6c0736-8974-4627-bd59-7ff29ce45ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compare witht the originals\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "for k in range(nrows*ncols):\n",
    "    plt.subplot(nrows, ncols, k+1)\n",
    "    plt.imshow(np.reshape(faces_zero_centered[k, :], (64, 64)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q2p1": {
     "name": "q2p1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> subf = faces[:20, :]\n>>> assert np.all(abs(np.mean(remove_mean(subf), axis=1) < 0.2))\n",
         "failure_message": "Check your implementation",
         "hidden": false,
         "locked": false,
         "success_message": "Good, results seem zero-centered"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2p2": {
     "name": "q2p2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X = np.random.randn(10, 20)\n>>> (eva, ev) = covariance_sorted_eigv(X)\n>>> assert np.all(np.diff(eva) <= 0)\n",
         "failure_message": "Eigenvalues are not sorted",
         "hidden": false,
         "locked": false,
         "success_message": "Good, eigenvalues are properly sorted"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
